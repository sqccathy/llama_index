{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/memory/Mem0Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mem0\n",
    "\n",
    "Mem0 (pronounced ‚Äúmem-zero‚Äù) enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences and traits and continuously updates over time, making it ideal for applications like customer support chatbots and AI assistants.\n",
    "\n",
    "Mem0 offers two powerful ways to leverage our technology: our [managed platform](https://docs.mem0.ai/platform/overview) and our [open source solution](https://docs.mem0.ai/open-source/quickstart)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ü¶ô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-memory-mem0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup with Mem0 Platform\n",
    "\n",
    "Set your Mem0 Platform API key as an environment variable. You can replace `<your-mem0-api-key>` with your actual API key:\n",
    "\n",
    "> Note: You can obtain your Mem0 Platform API key from the [Mem0 Platform](https://app.mem0.ai/login).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MEM0_API_KEY\"] = \"<your-mem0-api-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `from_client` (for Mem0 platform API): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.memory.mem0 import Mem0Memory\n",
    "\n",
    "context = {\"user_id\": \"test_user_1\"}\n",
    "memory_from_client = Mem0Memory.from_client(\n",
    "    context=context,\n",
    "    api_key=\"<your-api-key>\",\n",
    "    search_msg_limit=4,  # Default is 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mem0 Context is used to identify the user, agent or the conversation in the Mem0. It is required to be passed in the at least one of the fields in the `Mem0Memory` constructor.\n",
    "\n",
    "`search_msg_limit` is optional, default is 5. It is the number of messages from the chat history to be used for memory retrieval from Mem0. More number of messages will result in more context being used for retrieval but will also increase the retrieval time and might result in some unwanted results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `from_config` (for Mem0 OSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"<your-api-key>\"\n",
    "config = {\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"qdrant\",\n",
    "        \"config\": {\n",
    "            \"collection_name\": \"test_9\",\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": 6333,\n",
    "            \"embedding_model_dims\": 1536,  # Change this according to your local model's dimensions\n",
    "        },\n",
    "    },\n",
    "    \"llm\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"config\": {\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"temperature\": 0.2,\n",
    "            \"max_tokens\": 1500,\n",
    "        },\n",
    "    },\n",
    "    \"embedder\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"config\": {\"model\": \"text-embedding-3-small\"},\n",
    "    },\n",
    "    \"version\": \"v1.1\",\n",
    "}\n",
    "memory_from_config = Mem0Memory.from_config(\n",
    "    context=context,\n",
    "    config=config,\n",
    "    search_msg_limit=4,  # Default is 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mem0 for Function Calling Agents\n",
    "\n",
    "Use `Mem0` as memory for `FunctionCallingAgents`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_fn(name: str):\n",
    "    \"\"\"Call the provided name.\n",
    "    Args:\n",
    "        name: str (Name of the person)\n",
    "    \"\"\"\n",
    "    print(f\"Calling... {name}\")\n",
    "\n",
    "\n",
    "def email_fn(name: str):\n",
    "    \"\"\"Email the provided name.\n",
    "    Args:\n",
    "        name: str (Name of the person)\n",
    "    \"\"\"\n",
    "    print(f\"Emailing... {name}\")\n",
    "\n",
    "\n",
    "call_tool = FunctionTool.from_defaults(fn=call_fn)\n",
    "email_tool = FunctionTool.from_defaults(fn=email_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = FunctionCallingAgent.from_tools(\n",
    "    [call_tool, email_tool],\n",
    "    llm=llm,\n",
    "    memory=memory_from_client,  # can be memory_from_config\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 47f7d617-4756-4f7c-8858-baf6d8e0ce3a. Step input: Hi, My name is Mayank.\n",
      "Added user message to memory: Hi, My name is Mayank.\n",
      "=== LLM Response ===\n",
      "Hello Mayank! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Hi, My name is Mayank.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step f08de932-bd3d-4ded-a701-5b2c6bc62788. Step input: My preferred way of communication would be Email.\n",
      "Added user message to memory: My preferred way of communication would be Email.\n",
      "=== LLM Response ===\n",
      "Got it, Mayank! Your preferred way of communication is email. How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"My preferred way of communication would be Email.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step ccf8aae2-4e0e-459e-be6c-ead62d330c96. Step input: Send me an update of your product.\n",
      "Added user message to memory: Send me an update of your product.\n",
      "=== Calling Function ===\n",
      "Calling function: email_fn with args: {\"name\": \"Mayank\"}\n",
      "Emailing... Mayank\n",
      "=== Function Output ===\n",
      "None\n",
      "> Running step e26553f6-6b3f-46d1-aa84-9ba34879461f. Step input: None\n",
      "=== LLM Response ===\n",
      "I've sent you an update of our product via email. If you have any other questions or need further assistance, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Send me an update of your product.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mem0 for Chat Engines\n",
    "\n",
    "Use `Mem0` as memory to `SimpleChatEngine`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chat engine\n",
    "from llama_index.core.chat_engine.simple import SimpleChatEngine\n",
    "\n",
    "agent = SimpleChatEngine.from_defaults(\n",
    "    llm=llm, memory=memory_from_client  # can be memory_from_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mayank! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Hi, My name is mayank\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That sounds exciting! San Francisco has a lot to offer. If you need any recommendations on places to visit or things to do, feel free to ask. Safe travels!\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"I am planning to visit SF tommorow.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since you're planning to visit San Francisco tomorrow, it might be best to schedule meetings either in the morning before you start exploring or in the late afternoon or evening after you've had some time to enjoy the city. This way, you can make the most of your visit without feeling rushed. Let me know if you need help with anything else!\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"What would be a suitable time to schedule a meeting tommorow?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mem0 for ReAct Agents\n",
    "\n",
    "Use `Mem0` as memory for `ReActAgent`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "agent = ReActAgent.from_tools(\n",
    "    [call_tool, email_tool],\n",
    "    llm=llm,\n",
    "    memory=memory_from_client,  # can be memory_from_config\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step fb3acef3-f806-493e-838f-eae950df54f4. Step input: Hi, My name is Mayank.\n",
      "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: Hello Mayank! How can I assist you today?\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Hi, My name is Mayank.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step e1dfafe0-7fb1-4f37-a3bd-a4d93ef7e19f. Step input: My preferred way of communication would be Email.\n",
      "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: Got it, Mayank! If you need to communicate or schedule anything, I'll make sure to use email as your preferred method. Let me know if there's anything specific you need help with!\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"My preferred way of communication would be Email.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step e4a6f24c-0008-41d7-8fe9-53d41f26cd87. Step input: Send me an update of your product.\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me send an update via email.\n",
      "Action: email_fn\n",
      "Action Input: {'name': 'Mayank'}\n",
      "\u001b[0mEmailing... Mayank\n",
      "\u001b[1;3;34mObservation: None\n",
      "\u001b[0m> Running step aec65a36-eabf-473b-8572-0cb02b25969b. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have sent the email to Mayank with the product update. I can now confirm this action.\n",
      "Answer: I have sent you an update of our product via email. Please check your inbox. Let me know if there's anything else you need!\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Send me an update of your product.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step c3d61bd9-6415-49b3-94ba-8cb0ee717e3a. Step input: First call me and then communicate me requirements.\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n",
      "Action: call_fn\n",
      "Action Input: {'name': 'Mayank'}\n",
      "\u001b[0mCalling... Mayank\n",
      "\u001b[1;3;34mObservation: None\n",
      "\u001b[0m> Running step 6a594cb7-af31-48e3-b965-7fef88b03084. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: Since the call did not go through, I will proceed with the next step, which is to communicate via email as per your preference.\n",
      "Action: email_fn\n",
      "Action Input: {'name': 'Mayank'}\n",
      "\u001b[0mEmailing... Mayank\n",
      "\u001b[1;3;34mObservation: None\n",
      "\u001b[0m> Running step 72e96d48-afcc-48fc-83d7-5c0232dd7a92. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I have attempted to call and email you, but there seems to be no response from the tools. I will provide the information here instead.\n",
      "Answer: I attempted to call you, but it seems there was an issue. I'll proceed with providing the requirements update here. Please let me know if you have any specific requirements or updates you need, and I'll be happy to assist you!\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"First call me and then communicate me requirements.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
